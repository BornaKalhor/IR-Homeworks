{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of tf-idf.ipynb","provenance":[],"collapsed_sections":["TZE3fS0XkAWm","ZYGqsIy5kSuP","dA5wf-NTHUxQ"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TZE3fS0XkAWm"},"source":["#**some preprocessing stuff**"]},{"cell_type":"code","metadata":{"id":"7wni2z4utvC_"},"source":["import nltk\n","import string\n","import pickle\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1yTMbGKZv8Uh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637748640816,"user_tz":-210,"elapsed":4,"user":{"displayName":"aghaghia mohammadi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11624625006862481904"}},"outputId":"a80f3c8b-9199-4d3a-82ec-17bc2a0ac3ba"},"source":["nltk.download('stopwords')\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"WFObLvVUvLp0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637748640817,"user_tz":-210,"elapsed":4,"user":{"displayName":"aghaghia mohammadi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11624625006862481904"}},"outputId":"8e72a2b2-5ebe-4a75-83bb-3df25a27be35"},"source":["drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"X2LR-kyLtgzP"},"source":["def generate_tokens(txt: str) -> list:\n","    \"\"\"\n","    Processes a string and returns a list of tokens.\n","    :param txt: The string to process.\n","    :return: A list of tokens.\n","    \"\"\"\n","    stop_words = stopwords.words('english') + list(string.punctuation)\n","    stemmer = PorterStemmer()\n","    tokens = [stemmer.stem(word.lower()) for word in nltk.word_tokenize(txt) \\\n","               if word.lower() not in stop_words and word.isalpha()]\n","    return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"93hKBaR0trO4"},"source":["def create_revert_index(tokens: list) -> dict:\n","    \"\"\"\n","    Creates a reverse index of the tokens.\n","    :param tokens: A list of tokens.\n","    :return: A dictionary of tokens.\n","    \"\"\"\n","    revent_index = {}\n","    for index, token in enumerate(tokens):\n","        if token not in revent_index:\n","            revent_index[token] = {\n","                'repeat': 1,\n","                'indexes': [\n","                    index # this is not actually the index of token in tokens, \n","                                        # this must be set to token index in orginal string???\n","                ]\n","            }\n","        else:\n","            revent_index[token]['repeat'] += 1\n","            revent_index[token]['indexes'].append(index)\n","\n","    return revent_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJ6g66kJt0B7"},"source":["def process_data(path) -> dict:\n","    \"\"\"\n","        for each movie in the dataset:\n","            get the summary text\n","            generate tokens\n","            generate revert index\n","    \"\"\"\n","    processed = {}\n","    print('Started generating...')\n","    with open(path) as file:\n","        for line in file:\n","            movie_id, summary = line.split('\\t')\n","            tokens = generate_tokens(summary)\n","            index = create_revert_index(tokens)\n","            processed[movie_id] = {\n","                'summary': summary,\n","                'tokens': tokens,\n","                'index': index\n","            }\n","            # print(processed[movie_id])\n","    print('Finished...')\n","    return processed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t68MFc2KwC1Y","executionInfo":{"status":"ok","timestamp":1637736763417,"user_tz":-210,"elapsed":348251,"user":{"displayName":"aghaghia mohammadi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11624625006862481904"}},"outputId":"be38a52a-e192-48cb-c2f3-a7067d9b2ee1"},"source":["processed = process_data(\"/content/drive/MyDrive/Information Retrival/plot_summaries.txt\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Started generating...\n","Finished...\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3t-2O3X_DqVV","executionInfo":{"status":"ok","timestamp":1637742820626,"user_tz":-210,"elapsed":779,"user":{"displayName":"aghaghia mohammadi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11624625006862481904"}},"outputId":"3b8e51fa-2f51-4d21-fd2f-b5960934d0bd"},"source":["query = process_data(\"/content/drive/MyDrive/Information Retrival/queries.txt\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Started generating...\n","Finished...\n"]}]},{"cell_type":"markdown","metadata":{"id":"qqASaKPNkJkN"},"source":["merge index of queries and documents:"]},{"cell_type":"code","metadata":{"id":"WMgAQd3SGh0y"},"source":["processed.update(query)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYGqsIy5kSuP"},"source":["#**Calculate TFIDF:**"]},{"cell_type":"markdown","metadata":{"id":"CGSXeFY8t190"},"source":["calculate Document Frequency(DF) by adding each document id that contains specific word "]},{"cell_type":"code","metadata":{"id":"qj6RzdJ2ceJ2"},"source":["DF = {}\n","for id, detail in processed.items():\n","  for word, r in detail['index'].items():\n","    try:\n","      DF[word].add(id)\n","    except:\n","      DF[word] = {id}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vWfQ9emy2ool"},"source":["calculate tfidf:"]},{"cell_type":"markdown","metadata":{"id":"JeO56LF9sEU2"},"source":["**statistics = {'movieid'= {words}}**</br>\n","**words = {'tf', 'tfidf'}** </br>\n","idf is not considered as an important key cause: 1. we have it in another dict 2. it's not a reliable factor(it'll chsnge by adding a new movie plot!)"]},{"cell_type":"markdown","metadata":{"id":"4NRzsZByCU2K"},"source":["due to final multiplication of tf and idf, it's unnecessary to calculate idf for all words in a doc or query </br>\n","if we don't have X word in the Document its tf will be 0 and then result of multiplication will be 0;"]},{"cell_type":"code","metadata":{"id":"-mqC_4asy4Qi"},"source":["import math\n","statistics = {}\n","for id, detail in processed.items():\n","  words = {}\n","  for word, r in detail['index'].items():\n","    idf = math.log(len(DF)/len(DF[word]), 10)\n","    tf = math.log(r['repeat'], 10) + 1\n","    words[word] = {\n","      'tf': tf,\n","      'tfidf': tf * idf \n","    }\n","  statistics[id] = words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzyfF6UQHBAL"},"source":["#**Cosine similarity**\n","between queries and documents"]},{"cell_type":"markdown","metadata":{"id":"xcruEe6LvnN6"},"source":["normalize each plot vector by dividing tfidf of each word by sum of tfidf squares of all words.</br>\n","**tfidf of a word = wi</br>\n","wi/sum(wi)**</br>\n","update statics and add normolized scores to each word statistics: **words = {'tf', 'tfidf', 'normalized'}**"]},{"cell_type":"code","metadata":{"id":"aI_btjx7HUDt"},"source":["for id, words in statistics.items():\n","  nrmlzdSum = sum([math.sqrt(stat['tfidf']) for word, stat in words.items()])\n","  for word, stat in words.items():\n","    statistics[id][word]['normalized'] = stat['tfidf']/nrmlzdSum "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5rZeovRoxt30"},"source":["calculate the semilarity bet. queries and plots by multiplying each word that exist in query to all words in plots."]},{"cell_type":"code","metadata":{"id":"fpeB7FqgbbN6"},"source":["output = {}\n","for i in range(1,11):\n","  result = {}\n","  for docs in set(statistics)-{'1','2','3','4','5','6','7','8','9','10'}:\n","    result[docs] = sum([stat['normalized']*statistics[str(docs)][word]['normalized']\\\n","                   for word, stat in statistics[str(i)].items()\\\n","                   if statistics[str(docs)].get(word)!=None])\n","  output[str(i)] = result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0lYVpsZwxNac"},"source":["sort results and find 10 most similar plots to each query"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C-6wxl9xgkGh","executionInfo":{"status":"ok","timestamp":1637751628592,"user_tz":-210,"elapsed":457,"user":{"displayName":"aghaghia mohammadi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11624625006862481904"}},"outputId":"66fed1ba-a256-4c3e-9fcf-893be00a1513"},"source":["import heapq\n","from operator import itemgetter\n","topitems = []\n","for i in range(1, 11):\n"," topitems.append(heapq.nlargest(10, output[str(i)].keys(), key=itemgetter(1)))\n","print(topitems)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['29443845', '9951615', '29991802', '19310709', '6954675', '9962261', '2918617', '29320666', '893465', '4965004'], ['29443845', '9951615', '29991802', '19310709', '6954675', '9962261', '2918617', '29320666', '893465', '4965004'], ['29443845', '9951615', '29991802', '19310709', '6954675', '9962261', '2918617', '29320666', '893465', '4965004'], ['29443845', '9951615', '29991802', '19310709', '6954675', '9962261', '2918617', '29320666', '893465', '4965004'], ['29443845', '9951615', '29991802', '19310709', '6954675', '9962261', '2918617', '29320666', '893465', '4965004'], ['29443845', '9951615', '29991802', '19310709', '6954675', '9962261', '2918617', '29320666', '893465', '4965004'], ['29443845', '9951615', '29991802', '19310709', '6954675', '9962261', '2918617', '29320666', '893465', '4965004'], ['29443845', '9951615', '29991802', '19310709', '6954675', '9962261', '2918617', '29320666', '893465', '4965004'], ['29443845', '9951615', '29991802', '19310709', '6954675', '9962261', '2918617', '29320666', '893465', '4965004'], ['29443845', '9951615', '29991802', '19310709', '6954675', '9962261', '2918617', '29320666', '893465', '4965004']]\n"]}]},{"cell_type":"markdown","metadata":{"id":"RVxBdc-CxYCa"},"source":["Finally..\n","create 10 files to save result of queries."]},{"cell_type":"code","metadata":{"id":"MgFiXJwHmyMo"},"source":["for i in range(1, 11):\n","  p = '/content/drive/MyDrive/Information Retrival/query'+str(i)+'.txt'\n","  with open(p, 'w') as writefile:\n","    writefile.write('Top 10 of similar movie plot with \"'+ processed[str(i)]['summary']+ '\"')\n","    for docId in queryResult:\n","      writefile.write(docId + \"    \" + processed[docId]['summary'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dA5wf-NTHUxQ"},"source":["# **Junk**"]},{"cell_type":"code","metadata":{"id":"VxnJcTi8XeD8"},"source":["from itertools import chain\n","uniqueWords = set(chain.from_iterable(plot['index'].keys() for plot in processed.values()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RGhYPdX4tZMG"},"source":["creating a count vector by mean if we have: </br>\n","...     'This is the first document.', </br>\n","...     'This document is the second document.',</br>\n","...     'And this is the third one.',</br>\n","...     'Is this the first document?',</br>\n","generate a vocab array that looks like: </br>\n","array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n","       'this'], ...)</br>\n","and finally our count vector will be: </br>\n","[[0 1 1 1 0 0 1 0 1]</br>\n"," [0 2 0 1 0 1 1 0 1]</br>\n"," [1 0 0 1 1 0 1 1 1]</br>\n"," [0 1 1 1 0 0 1 0 1]]"]},{"cell_type":"code","metadata":{"id":"o1sdC86zyent"},"source":["import numpy as np\n","vocabulary = {b: a for a, b in enumerate(uniqueWords)}\n","movies = {b:a for a,b in enumerate(processed.keys())}\n","countVector = []\n","for id, detail in processed.items():\n","  movie = np.zeros(len(uniqueWords))\n","  for word, r in detail['index'].items():\n","    movie[vocabulary[word]]=r['repeat']\n","    countVector.append(movie)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kfQ_1vf_H_GP"},"source":["from itertools import chain\n","def UniqueWords(dic) -> list:\n","    # Stores the list of unique keys\n","    res = list(set(chain.from_iterable(plot['index'].keys() for plot in dic.values())))\n","    return res\n","    # # Print the list\n","    # print(str(res))"],"execution_count":null,"outputs":[]}]}